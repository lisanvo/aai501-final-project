{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Lisa Vo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-623ecb49dce2cff2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Classification Using Scikit-learn\n",
    "\n",
    "We're going to explore in more detail some of the functionality provided by scikit-learn (`sklearn`) for doing machine learning, and specifically in this assignment, classification.  We will use the World Happiness Report (WHR) data, bringing in some additional information that will enable us to formulate a classification problem to predict categorical labels on the dataset.\n",
    "\n",
    "Execute the code cell below to import some modules and read in and preprocess the WHR data.  The last line in the code cell below returns the head of the basic WHR dataframe, to show you what is in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-528dcd796b7a9020",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>LogGDP</th>\n",
       "      <th>Support</th>\n",
       "      <th>Life</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2008</td>\n",
       "      <td>3.723590</td>\n",
       "      <td>0.517637</td>\n",
       "      <td>0.258195</td>\n",
       "      <td>7.168690</td>\n",
       "      <td>0.450662</td>\n",
       "      <td>49.209663</td>\n",
       "      <td>0.718114</td>\n",
       "      <td>0.181819</td>\n",
       "      <td>0.881686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2009</td>\n",
       "      <td>4.401778</td>\n",
       "      <td>0.583926</td>\n",
       "      <td>0.237092</td>\n",
       "      <td>7.333790</td>\n",
       "      <td>0.552308</td>\n",
       "      <td>49.624432</td>\n",
       "      <td>0.678896</td>\n",
       "      <td>0.203614</td>\n",
       "      <td>0.850035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2010</td>\n",
       "      <td>4.758381</td>\n",
       "      <td>0.618265</td>\n",
       "      <td>0.275324</td>\n",
       "      <td>7.386629</td>\n",
       "      <td>0.539075</td>\n",
       "      <td>50.008961</td>\n",
       "      <td>0.600127</td>\n",
       "      <td>0.137630</td>\n",
       "      <td>0.706766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2011</td>\n",
       "      <td>3.831719</td>\n",
       "      <td>0.611387</td>\n",
       "      <td>0.267175</td>\n",
       "      <td>7.415019</td>\n",
       "      <td>0.521104</td>\n",
       "      <td>50.367298</td>\n",
       "      <td>0.495901</td>\n",
       "      <td>0.175329</td>\n",
       "      <td>0.731109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2012</td>\n",
       "      <td>3.782938</td>\n",
       "      <td>0.710385</td>\n",
       "      <td>0.267919</td>\n",
       "      <td>7.517126</td>\n",
       "      <td>0.520637</td>\n",
       "      <td>50.709263</td>\n",
       "      <td>0.530935</td>\n",
       "      <td>0.247159</td>\n",
       "      <td>0.775620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  year  Happiness  Positive  Negative    LogGDP   Support  \\\n",
       "0  Afghanistan  2008   3.723590  0.517637  0.258195  7.168690  0.450662   \n",
       "1  Afghanistan  2009   4.401778  0.583926  0.237092  7.333790  0.552308   \n",
       "2  Afghanistan  2010   4.758381  0.618265  0.275324  7.386629  0.539075   \n",
       "3  Afghanistan  2011   3.831719  0.611387  0.267175  7.415019  0.521104   \n",
       "4  Afghanistan  2012   3.782938  0.710385  0.267919  7.517126  0.520637   \n",
       "\n",
       "        Life   Freedom  Generosity  Corruption  \n",
       "0  49.209663  0.718114    0.181819    0.881686  \n",
       "1  49.624432  0.678896    0.203614    0.850035  \n",
       "2  50.008961  0.600127    0.137630    0.706766  \n",
       "3  50.367298  0.495901    0.175329    0.731109  \n",
       "4  50.709263  0.530935    0.247159    0.775620  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "dfraw = pd.read_excel('WHR2018Chapter2OnlineData.xls', sheet_name='Table2.1')\n",
    "cols_to_include = ['country', 'year', 'Life Ladder', \n",
    "                   'Positive affect','Negative affect',\n",
    "                   'Log GDP per capita', 'Social support',\n",
    "                   'Healthy life expectancy at birth', \n",
    "                   'Freedom to make life choices', \n",
    "                   'Generosity', 'Perceptions of corruption']\n",
    "renaming = {'Life Ladder': 'Happiness', \n",
    "            'Log GDP per capita': 'LogGDP', \n",
    "            'Social support': 'Support', \n",
    "            'Healthy life expectancy at birth': 'Life', \n",
    "            'Freedom to make life choices': 'Freedom', \n",
    "            'Perceptions of corruption': 'Corruption', \n",
    "            'Positive affect': 'Positive', \n",
    "            'Negative affect': 'Negative'}\n",
    "df = dfraw[cols_to_include].rename(renaming, axis=1)\n",
    "key_vars = ['Happiness', 'LogGDP', 'Support', 'Life', 'Freedom', 'Generosity', 'Corruption', 'Positive', 'Negative']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-114766d8d149ad20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1.\n",
    "\n",
    "We will first augment the core WHR dataset that we have been working with to bring in some additional information that is included in a different worksheet in the WHR spreadsheet.  Since this is mostly about data processing rather than machine learning, simply execute the next two code cells below.  But study each line of code and the associated comments, and then examine the head of the new dataframe named ```df2``` to understand what has been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4961999f914bb49e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Middle East and North Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                        region\n",
       "0  Afghanistan                    South Asia\n",
       "1      Albania    Central and Eastern Europe\n",
       "2      Algeria  Middle East and North Africa\n",
       "3       Angola            Sub-Saharan Africa\n",
       "4    Argentina   Latin America and Caribbean"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data from SupportingFactors worksheet into a new dataframe dfsupp\n",
    "dfsupp = pd.read_excel('WHR2018Chapter2OnlineData.xls', sheet_name='SupportingFactors')\n",
    "\n",
    "# extract out region information from SupportingFactors dataframe\n",
    "regions = dfsupp[['country', 'Region indicator']].rename({'Region indicator': 'region'}, axis=1)\n",
    "\n",
    "# examine head of regions dataframe -- each country has an associated world region\n",
    "regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04c2e566637e8957",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>LogGDP</th>\n",
       "      <th>Support</th>\n",
       "      <th>Life</th>\n",
       "      <th>Freedom</th>\n",
       "      <th>Generosity</th>\n",
       "      <th>Corruption</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>3.806614</td>\n",
       "      <td>0.580873</td>\n",
       "      <td>0.301283</td>\n",
       "      <td>7.419697</td>\n",
       "      <td>0.517146</td>\n",
       "      <td>50.838271</td>\n",
       "      <td>0.544895</td>\n",
       "      <td>0.118428</td>\n",
       "      <td>0.826794</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>4.988791</td>\n",
       "      <td>0.642628</td>\n",
       "      <td>0.303256</td>\n",
       "      <td>9.247059</td>\n",
       "      <td>0.723204</td>\n",
       "      <td>68.027213</td>\n",
       "      <td>0.626155</td>\n",
       "      <td>-0.105019</td>\n",
       "      <td>0.859691</td>\n",
       "      <td>Central and Eastern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <td>5.555004</td>\n",
       "      <td>0.616524</td>\n",
       "      <td>0.265460</td>\n",
       "      <td>9.501728</td>\n",
       "      <td>0.804633</td>\n",
       "      <td>64.984461</td>\n",
       "      <td>0.536398</td>\n",
       "      <td>-0.208236</td>\n",
       "      <td>0.661478</td>\n",
       "      <td>Middle East and North Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <td>4.420299</td>\n",
       "      <td>0.613339</td>\n",
       "      <td>0.351173</td>\n",
       "      <td>8.713935</td>\n",
       "      <td>0.737973</td>\n",
       "      <td>51.729801</td>\n",
       "      <td>0.455957</td>\n",
       "      <td>-0.077940</td>\n",
       "      <td>0.867018</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Argentina</th>\n",
       "      <td>6.406131</td>\n",
       "      <td>0.840998</td>\n",
       "      <td>0.273187</td>\n",
       "      <td>9.826051</td>\n",
       "      <td>0.906080</td>\n",
       "      <td>66.764205</td>\n",
       "      <td>0.753122</td>\n",
       "      <td>-0.154544</td>\n",
       "      <td>0.844038</td>\n",
       "      <td>Latin America and Caribbean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Happiness  Positive  Negative    LogGDP   Support       Life  \\\n",
       "country                                                                     \n",
       "Afghanistan   3.806614  0.580873  0.301283  7.419697  0.517146  50.838271   \n",
       "Albania       4.988791  0.642628  0.303256  9.247059  0.723204  68.027213   \n",
       "Algeria       5.555004  0.616524  0.265460  9.501728  0.804633  64.984461   \n",
       "Angola        4.420299  0.613339  0.351173  8.713935  0.737973  51.729801   \n",
       "Argentina     6.406131  0.840998  0.273187  9.826051  0.906080  66.764205   \n",
       "\n",
       "              Freedom  Generosity  Corruption                        region  \n",
       "country                                                                      \n",
       "Afghanistan  0.544895    0.118428    0.826794                    South Asia  \n",
       "Albania      0.626155   -0.105019    0.859691    Central and Eastern Europe  \n",
       "Algeria      0.536398   -0.208236    0.661478  Middle East and North Africa  \n",
       "Angola       0.455957   -0.077940    0.867018            Sub-Saharan Africa  \n",
       "Argentina    0.753122   -0.154544    0.844038   Latin America and Caribbean  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the mean values of all the WHR data for each country, averaging over all years in the dataset\n",
    "dfmean = df.groupby('country').mean().drop('year', axis=1)\n",
    "\n",
    "# merge the mean WHR data with the region information extracted previously\n",
    "df2 = pd.merge(dfmean, regions, on='country').dropna()\n",
    "\n",
    "# set the index of df2 to be the country name\n",
    "df2.set_index('country', inplace=True)\n",
    "\n",
    "# examine head of df2 dataframe -- mean WHR values for each country, along with associated regions\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ebf9c19dc4374f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2.\n",
    "\n",
    "It is this new dataframe ```df2``` that we want to use for machine learning.  For each country in the dataset, we have a set of mean numerical values ('Happiness', 'Positive', 'Negative', etc., which are all listed in the variable ```key_vars``` defined above) and a categorical value ('region').  We would like to know if the raw numerical data are  predictive of the region.  In other words, if someone gave you a set of numerical data on Happiness, etc. for an unknown country, would you be able to predict what region of the world it might be located in?  This is an example of classification, where we will train a model based on the numerical data and the associated labels (regions).\n",
    "\n",
    "In order to proceed, we first want to extract and process some data from our ```df2``` dataframe.  We need to separate the data into two parts:\n",
    "* the region data that we want to be able to predict (we'll call it ```y```)\n",
    "* the WHR numerical data that we want to use as input to our prediction (we'll call it ```x```)\n",
    "\n",
    "Again, our goal is to build a classifier that we will train on a subset of the WHR numerical data (x) and the region data (y), so that we can predict regions from data for countries that we have not trained our model on.\n",
    "\n",
    "In the code cell below:\n",
    "* Extract the subset of ```df2``` associated with the columns in ```key_vars``` and assign it to the variable ```x```.\n",
    "* Extract the subset of ```df2``` associated with the region column, and assign it to the variable ```y```.\n",
    "* Print the shape of both `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 5% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01ec5c8a944da95a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x = (152, 9)\n",
      "Shape of y = (152,)\n"
     ]
    }
   ],
   "source": [
    "# Get subset by intersection of df2 and key_vars\n",
    "x = df2[df2.columns.intersection(key_vars)]\n",
    "\n",
    "# Get region data\n",
    "y = df2['region']\n",
    "\n",
    "# Print shape of x and y\n",
    "print(f'Shape of x = {x.shape}')\n",
    "print(f'Shape of y = {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7c2b22581801780",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3.\n",
    "\n",
    "You should note that the shape of ```x``` is (152, 9) and the shape of ```y``` is (152,).  There are 152 samples (countries), and 9 features (each of the key_vars) that we are making predictions from.\n",
    "\n",
    "Because the numerical data columns in ```x``` represent different quantities and have different scales, it is useful to preprocess the data to remove that source of variation.  `sklearn` provides various utilities for such preprocessing.  We will use one here called ```StandardScaler```, which will transform a data set so that each resulting column has zero mean and unit standard deviation.\n",
    "\n",
    "Carrying out this scaling is a little complicated if we want to maintain the basic structure of the dataframe, so we have provided the relevant code in the next code cell below.  (The code examples describing StandardScaler in the [sklearn documentation](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler) typically just extract out the numerical values in numpy arrays, but for this exercise, we'd like to keep the labels together in a dataframe.)\n",
    "\n",
    "Please perform the following steps in the below graded cell:\n",
    "* imports the `StandardScaler` object\n",
    "* creates and fits a `StandardScaler` object to our dataframe ```x```\n",
    "* creates a new dataframe ```x_scaled``` that contains the scaled (transformed) data, using the column and index labels from the unscaled dataframe ```x```\n",
    "* prints out the mean and standard deviation of each column of ```x_scaled```\n",
    "* peeks at the head of the new dataframe ```x_scaled```\n",
    "\n",
    "In examining the output, check that the means of each column have been scaled to nearly zero (to within a very small tolerance) and the standard deviations have been scaled to one. Some of the very small numbers might be printed out in scientific notation, where a number like ```1.928282e-16``` means ```1.928282 * 10**(-16)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 20% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54c74ff720e1fb98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 0    1.782200e-16\n",
      "1    1.811417e-16\n",
      "2    2.337312e-16\n",
      "3    6.135443e-17\n",
      "4   -2.337312e-16\n",
      "5   -5.843279e-17\n",
      "6    6.748987e-16\n",
      "7    1.168656e-17\n",
      "8    9.349247e-17\n",
      "dtype: float64\n",
      "Standard deviation = 0    1.003306\n",
      "1    1.003306\n",
      "2    1.003306\n",
      "3    1.003306\n",
      "4    1.003306\n",
      "5    1.003306\n",
      "6    1.003306\n",
      "7    1.003306\n",
      "8    1.003306\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.443128</td>\n",
       "      <td>-1.262731</td>\n",
       "      <td>0.471370</td>\n",
       "      <td>-1.438896</td>\n",
       "      <td>-2.425953</td>\n",
       "      <td>-1.333584</td>\n",
       "      <td>-1.397623</td>\n",
       "      <td>0.735439</td>\n",
       "      <td>0.451854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.360792</td>\n",
       "      <td>-0.638194</td>\n",
       "      <td>0.499009</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>-0.681799</td>\n",
       "      <td>0.776161</td>\n",
       "      <td>-0.776670</td>\n",
       "      <td>-0.719736</td>\n",
       "      <td>0.632648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157600</td>\n",
       "      <td>-0.902184</td>\n",
       "      <td>-0.030449</td>\n",
       "      <td>0.262588</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.402698</td>\n",
       "      <td>-1.462554</td>\n",
       "      <td>-1.391919</td>\n",
       "      <td>-0.456675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.881273</td>\n",
       "      <td>-0.934399</td>\n",
       "      <td>1.170248</td>\n",
       "      <td>-0.381215</td>\n",
       "      <td>-0.556782</td>\n",
       "      <td>-1.224159</td>\n",
       "      <td>-2.077245</td>\n",
       "      <td>-0.543385</td>\n",
       "      <td>0.672914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936845</td>\n",
       "      <td>1.367958</td>\n",
       "      <td>0.077797</td>\n",
       "      <td>0.527632</td>\n",
       "      <td>0.866136</td>\n",
       "      <td>0.621142</td>\n",
       "      <td>0.193546</td>\n",
       "      <td>-1.042257</td>\n",
       "      <td>0.546624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.443128 -1.262731  0.471370 -1.438896 -2.425953 -1.333584 -1.397623   \n",
       "1 -0.360792 -0.638194  0.499009  0.054466 -0.681799  0.776161 -0.776670   \n",
       "2  0.157600 -0.902184 -0.030449  0.262588  0.007447  0.402698 -1.462554   \n",
       "3 -0.881273 -0.934399  1.170248 -0.381215 -0.556782 -1.224159 -2.077245   \n",
       "4  0.936845  1.367958  0.077797  0.527632  0.866136  0.621142  0.193546   \n",
       "\n",
       "          7         8  \n",
       "0  0.735439  0.451854  \n",
       "1 -0.719736  0.632648  \n",
       "2 -1.391919 -0.456675  \n",
       "3 -0.543385  0.672914  \n",
       "4 -1.042257  0.546624  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create and fit a StandardScaler object to x\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "\n",
    "# Create dataframe that contains scaled data, using unscaled dataframe x\n",
    "x_scaled = pd.DataFrame(scaler.transform(x))\n",
    "\n",
    "# Print mean and standard deviation of each column of x_scaled\n",
    "print(f'Mean = {x_scaled.mean()}')\n",
    "print(f'Standard deviation = {x_scaled.std()}')\n",
    "\n",
    "# Examine head of x_scaled\n",
    "x_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-120c4b66e20c858b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4.\n",
    "\n",
    "Now that the data have been preprocessed, we can begin with our classification analysis.  Let's begin by importing some additional things from `sklearn`.  Execute the code cell below to import:\n",
    "* the ```svm``` and ```tree``` submodules\n",
    "* the ```train_test_split``` function\n",
    "* the ```accuracy_score``` function\n",
    "\n",
    "We'll discuss in more detail below what each of these does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b61fbfb465b7fba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e4b3a68ad0f3755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 5.\n",
    "\n",
    "One of the convenience functions that we imported above is called ```train_test_split```.  As its name suggests, this function splits a dataset into separate training and testing sets.  The [online documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn-model-selection-train-test-split) indicates that it splits a dataset randomly, such that approximately 25% of the data winds up in the test set and the remaining 75% in the training set.  The documentation is a bit confusing, since the function can take a variable number of arrays as inputs; in our case, we want to split up 2 arrays (```x_scaled``` and ```y```) into coordinated test and train sets, so that the function will return a total of 4 subarrays (```x_train, x_test, y_train, y_test```).\n",
    "\n",
    "Because ```train_test_split``` generates random splits of the input data, each time we call the function we will get a different split.  Sometimes it is useful, for the purposes of code development, to be able to get reproducible random numbers or random splits, which can then be relaxed once one wishes to generate statistics over many random runs.     With ```train_test_split```, this can be accomplished by using the ```random_state``` option; if specified with that state as an integer, then the same random split will be generated each time the function is called (until one changes the value of the integer).  This is known as providing a seed to the pseudo-random number generator that is used by ```train_test_split```.\n",
    "\n",
    "You may enter and execute a call to ```train_test_split``` that takes ```x_scaled``` and ```y``` as inputs, along with the optional parameter ```random_state=0```, and returns the 4 data subsets mentioned above, to be named as ```x_train```, ```x_test```, ```y_train```, ```y_test```.  The online documentation provides an example of what such a function call looks like. After the function call, print the shapes of each of the four arrays that are returned.\n",
    "\n",
    "However there is a downside to perform standardization prior to train_test_split() which potentially leads to the information leak since the testing data distribution is used to scale the training dataset. In the code cell below, please perform train_test_split() first before applying StandardScaler().fit() only with the training dataset and then use it to transform the training dataset and the testing dataset separately. Use the separately scaled training dataset and the scaled testing dataset for the rest of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ba571e171fc1adf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 5% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before applying StandardScaler fit:\n",
      "Shape of x_train = (114, 9)\n",
      "Shape of x_test = (38, 9)\n",
      "Shape of y_train = (114,)\n",
      "Shape of y_test = (38,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, random_state=0)\n",
    "\n",
    "# Print shapes of each set (before applying StandardScaler().fit())\n",
    "print('Shape before applying StandardScaler fit:')\n",
    "print(f'Shape of x_train = {x_train.shape}')\n",
    "print(f'Shape of x_test = {x_test.shape}')\n",
    "print(f'Shape of y_train = {y_train.shape}')\n",
    "print(f'Shape of y_test = {y_test.shape}')\n",
    "\n",
    "# Apply StandardScaler().fit() to training dataset\n",
    "x_scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "# Transform the training dataset and testing dataset\n",
    "x_train = pd.DataFrame(x_scaler.transform(x_train))\n",
    "x_test = pd.DataFrame(x_scaler.transform(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-effebc5e9940ed25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 6.\n",
    "\n",
    "Having split our datasets, we want to first train a classifier on our training data so that we can apply it to the testing data.  One way of assessing the performance of a classifier is by computing its accuracy on the test data, that is, what fraction of the test data are correctly predicted by the classifier.  Fortunately, `sklearn` provides a built-in function named ```accuracy_score``` that carries out this computation; we imported it above, and you can read more about it in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy_score#sklearn.metrics.accuracy_score).\n",
    "\n",
    "We imported above the ```svm``` and ```tree``` submodules from sklearn.  These provide support for Support Vector Machine (svm) and Decision Tree (tree) machine learning algorithms.  For more information, review the [Support Vector Machines (SVMs) documentation](https://scikit-learn.org/stable/modules/svm.html) and the [Decision Trees documentation](https://scikit-learn.org/stable/modules/tree.html).  Under the hood, these are very different types of algorithms.  Decision Trees try to formulate a series of yes/no questions based on the data that can distinguish the categories from one another.  SVMs, on the other hand, use techniques from geometry to find cuts through the data space to separate different categories from one another.  Understanding how these methods work in detail is beyond the scope of this exercise, but fortunately, despite the very different data structures and algorithms used internally, `sklearn` provides a uniform interface that lets us easily build these different sorts of classifiers and compare their performance.\n",
    "\n",
    "We will first consider SVMs, and then revisit the problem with Decision Trees.\n",
    "\n",
    "In the code cell below:\n",
    "* create a new ```svm.SVC()``` object and assign it to the variable ```clf1``` &mdash;  a call to ```svm.SVC()``` creates a Support Vector Classifier from the svm submodule, similar to what we did in the earlier exercise on hand-written digits\n",
    "* call the ```fit``` method on ```clf1``` with the `x` and `y` training data (i.e., training the model to associate ```x_train``` with ```y_train```)\n",
    "* call the ```predict``` method on ```clf1``` on the `x` testing data and assign the result to the variable ```predictions1```, in order to make predictions for those inputs\n",
    "* call the ```accuracy_score``` function on the `y` testing data and the test predictions you generated and assign the result to the variable ```score1```\n",
    "* print the value of ```score1```\n",
    "\n",
    "The accuracy score is a fraction between 0 and 1 indicating the fraction of predictions that match the true value in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 20% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa487bf06d148c8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "# Create SVC object\n",
    "clf1 = svm.SVC()\n",
    "\n",
    "# Call fit method on x and y training data\n",
    "clf1.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions on x testing data\n",
    "predictions1 = clf1.predict(x_test)\n",
    "\n",
    "# Calculate accuracy score on y testing data and test predictions\n",
    "score1 = accuracy_score(y_test, predictions1)\n",
    "\n",
    "# Print accuracy score\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d88f7095ce3f275",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 7.\n",
    "\n",
    "The accuracy score reported should be around 71% (0.71).  This means that approximately 29% of the countries in the test set had their regions mispredicted.  While that doesn't sound great, it could be that the WHR numerical data really are not always completely predictive of region; one could imagine some countries that are \"outliers\" in a particular region, and more closely resemble other regions based on the WHR indicators.\n",
    "\n",
    "In the below code cell, please loop over all the predicted and true values in the test set, and prints out the country name and predicted region when the prediction is incorrect.  An output line like: ```Sri Lanka : South Asia -> Sub-Saharan Africa``` means that Sri Lanka is actually part of the South Asia region but was predicted to be part of Sub-Saharan Africa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 10% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Israel : Middle East and North Africa -> Western Europe\n",
      "Sri Lanka : South Asia -> Sub-Saharan Africa\n",
      "Tajikistan : Commonwealth of Independent States -> Sub-Saharan Africa\n",
      "Yemen : Middle East and North Africa -> Sub-Saharan Africa\n",
      "Hong Kong S.A.R. of China : East Asia -> Western Europe\n",
      "Philippines : Southeast Asia -> Latin America and Caribbean\n",
      "Italy : Western Europe -> Central and Eastern Europe\n",
      "Slovenia : Central and Eastern Europe -> Western Europe\n",
      "Gabon : Sub-Saharan Africa -> Middle East and North Africa\n",
      "Azerbaijan : Commonwealth of Independent States -> Middle East and North Africa\n",
      "Malaysia : Southeast Asia -> Latin America and Caribbean\n"
     ]
    }
   ],
   "source": [
    "# Current index of predictions\n",
    "pred_idx = 0\n",
    "\n",
    "# Loop through y test data\n",
    "for country in y_test.index:\n",
    "    \n",
    "    # Check whether the true region is not equal to the predicted region\n",
    "    if y_test[country] is not predictions1[pred_idx]:\n",
    "        \n",
    "        # If they are not equal, get these regions\n",
    "        correct_region = y_test[country]\n",
    "        incorrect_region = predictions1[pred_idx]\n",
    "        \n",
    "        # Print country : true region -> incorrect region\n",
    "        print(f'{country} : {correct_region} -> {incorrect_region}')\n",
    "        \n",
    "    # Increment current index of predictions by 1\n",
    "    pred_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8cd9531f6db2bc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 8.\n",
    "\n",
    "It is often not obvious what specific algorithm will work best for a particular dataset, so it is good to be able to conduct numerical experiments to see how different methods perform (even if we might not fully understand *why* one method might work better than another).  Because `sklearn` provides a consistent interface to very different types of underlying algorithms, it is easy to build additional classifiers to carry out these kinds of comparisons.  Here, we will build a second classifier based on Decision Trees as supported by the ```tree``` module.  Decision Tree algorithms have an element of randomness to them, so a Decision Tree can also be constructed with a specified ```random_state```  such as an integer that seeds the random number generator.  Most of what we will do here is very similar to the code you wrote a few cells up when you built a SVC classifier.\n",
    "\n",
    "In the code cell below:\n",
    "\n",
    "* Create a new ```tree.DecisionTreeClassifier()``` object with the optional argument ```random_state=0```, and assign it to the variable ```clf2``` (`clf2` stands for \"classifier number 2\", so that we can compare with ```clf1``` above).\n",
    "* Call the ```fit``` method on ```clf2``` with the `x` and `y` training data (i.e., training the model to associate ```x_train``` with ```y_train```).\n",
    "* Call the ```predict``` method on ```clf2``` on the `x` testing data and assign the result to the variable ```predictions2```, in order to make predictions for those inputs.\n",
    "* Call the ```accuracy_score``` function on the `y` testing data and the test predictions you generated and assign the result to the variable ```score2```.\n",
    "* Print the value of ```score2```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 10% of the grade for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da49b9de6d360166",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "# Create DecisionTreeClassifier object\n",
    "clf2 = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Call fit method on x and y training data\n",
    "clf2.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions on x testing data\n",
    "predictions2 = clf2.predict(x_test)\n",
    "\n",
    "# Calculate accuracy score on y testing data and test predictions\n",
    "score2 = accuracy_score(y_test, predictions2)\n",
    "\n",
    "# Print accuracy score\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d4bfe43911002ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 9.\n",
    "\n",
    "We ran two classifiers &mdash; ```clf1``` (SVM) and ```clf2``` (Decision Tree) &mdash; on a particular random `train_test_split` of the full dataset.  We can't really reach any conclusions about the relative performance of the two methods just by considering one split.  Given that ```train_test_split``` can produce different random splits, let's write a little code to compare the two classifiers for different splits.\n",
    "\n",
    "In the code cell below, write some code to do the following:\n",
    "* write a Python `for` loop so that you can run through the loop 20 times\n",
    "* within each pass through the loop, do the following:\n",
    "    * call `test_train_split` on ```x_scaled``` and ```y``` to get new random instances of `x_train`, `x_test`, `y_train`, `y_test` -- in this case, you don't want to pass in a value for ```random_state``` since you want to get different random splits each time\n",
    "    * fit each of the classifiers `clf1` and `clf2` to `x_train` and `y_train`\n",
    "    * run predictions on each of the classifiers `clf1` and `clf2` on the `x` testing data\n",
    "    * compute the accuracy_score of each of the two classifiers on the test data and the test predictions you generated \n",
    "    * print the score of each classifier, as well as their difference (hint: ```print(score1, score2, score1-score2)``` to get just one line of output per iteration of the loop)\n",
    "    \n",
    "Execute the code you have written.  You should see it run through the loop 20 times, for different random data splits.  While the overall performance varies from run to run, you should probably see that the SVC classifier (```clf1```) generally performs a little bit better than the DecisionTree classifier (```clf2```).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 10% of the grade for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4b11d4e6c2398273",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894736842105263, 0.7631578947368421, 0.02631578947368418\n",
      "\n",
      "0.5526315789473685, 0.5526315789473685, 0.0\n",
      "\n",
      "0.7631578947368421, 0.631578947368421, 0.13157894736842113\n",
      "\n",
      "0.7105263157894737, 0.6842105263157895, 0.02631578947368418\n",
      "\n",
      "0.6842105263157895, 0.6578947368421053, 0.02631578947368418\n",
      "\n",
      "0.631578947368421, 0.5263157894736842, 0.10526315789473684\n",
      "\n",
      "0.6842105263157895, 0.631578947368421, 0.052631578947368474\n",
      "\n",
      "0.7368421052631579, 0.6052631578947368, 0.13157894736842102\n",
      "\n",
      "0.631578947368421, 0.631578947368421, 0.0\n",
      "\n",
      "0.6578947368421053, 0.6842105263157895, -0.02631578947368418\n",
      "\n",
      "0.6578947368421053, 0.5789473684210527, 0.07894736842105265\n",
      "\n",
      "0.6052631578947368, 0.5263157894736842, 0.07894736842105265\n",
      "\n",
      "0.6842105263157895, 0.5526315789473685, 0.13157894736842102\n",
      "\n",
      "0.7105263157894737, 0.7105263157894737, 0.0\n",
      "\n",
      "0.7631578947368421, 0.5263157894736842, 0.23684210526315796\n",
      "\n",
      "0.6578947368421053, 0.5526315789473685, 0.10526315789473684\n",
      "\n",
      "0.6578947368421053, 0.6842105263157895, -0.02631578947368418\n",
      "\n",
      "0.6842105263157895, 0.6842105263157895, 0.0\n",
      "\n",
      "0.631578947368421, 0.631578947368421, 0.0\n",
      "\n",
      "0.7368421052631579, 0.6052631578947368, 0.13157894736842102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20): \n",
    "    # Split data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_scaled, y)\n",
    "    \n",
    "    # Fit to each classifier\n",
    "    clf1.fit(x_train, y_train)\n",
    "    clf2.fit(x_train, y_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions1 = clf1.predict(x_test)\n",
    "    predictions2 = clf2.predict(x_test)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    score1 = accuracy_score(y_test, predictions1)\n",
    "    score2 = accuracy_score(y_test, predictions2)\n",
    "    \n",
    "    # Print accuracy scores and differences\n",
    "    print(f'{score1}, {score2}, {score1-score2}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01de5a27c9f1842e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 10.\n",
    "\n",
    "In the last code cell, you printed out the scores of the two classifiers for a small number of random splits, and examined the numerical output.  Perhaps you'd rather generate a visual summary of the relative performance of the two classifiers, for a larger number of runs.\n",
    "\n",
    "In the code cell below, copy and paste the code you wrote above and modify it to do the following:\n",
    "\n",
    "* prior to entering the `for` loop, initialize two empty lists named ```all_scores1``` and ```all_scores2``` that will be used to collect the scores of each classifier each time through the loop\n",
    "* run through the loop 1000 times instead of 20 as before\n",
    "* append the scores (```score1``` and ```score2```) to each of the lists used to contain all the scores\n",
    "* remove the print statement so that you don't get 1000 annoying print statements when you run the code\n",
    "* once the loop is finished, use the ```plt.hist``` function to plot histograms for ```all_scores1``` and ```all_scores2``` together in the same plot\n",
    "    * you can accomplish this by making two successive calls to the histogram function within the same code cell\n",
    "    * you might want to add options to change the number of bins for the histograms\n",
    "    * you should change the alpha value (opacity) of the histogram plots so that you can see both distributions, since at full opacity, the second one plotted will obscure the first one\n",
    "    * you should use the ``label`` option to label the datasets\n",
    "* After making your two calls to ```plt.hist```, you should call ``plt.legend`` to produce a legend on the plot that will identify the two datasets based on the label options that you added to your ```plt.hist``` calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graded Cell\n",
    "\n",
    "This cell is worth 20% of the grade for this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-28a13e824292104e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize empty lists\n",
    "all_scores1 = []\n",
    "all_scores2 = []\n",
    "\n",
    "for i in range(1000): \n",
    "    # Split data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_scaled, y)\n",
    "    \n",
    "    # Fit to each classifier\n",
    "    clf1.fit(x_train, y_train)\n",
    "    clf2.fit(x_train, y_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions1 = clf1.predict(x_test)\n",
    "    predictions2 = clf2.predict(x_test)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    score1 = accuracy_score(y_test, predictions1)\n",
    "    score2 = accuracy_score(y_test, predictions2)\n",
    "    \n",
    "    # Append scores to corresponding lists\n",
    "    all_scores1.append(score1)\n",
    "    all_scores2.append(score2)\n",
    "    \n",
    "# Plot histograms\n",
    "plt.hist(all_scores1, bins=20, label='SVM', alpha=0.5)\n",
    "plt.hist(all_scores2, bins=20, color='red', label='Decision Tree', alpha=0.5)\n",
    "plt.title('SVM Scores vs. Decision Tree Scores')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5096838c0f9fe754",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Just scratching the surface...\n",
    "\n",
    "This is just the start of what you can do with scikit-learn.  It is clear from the documentation that there are many different methods and algorithms for classification that are supported by the package, as well as different ways of optimizing and assessing the performance of different algorithms.  If you are motivated to explore further, feel free to continue below by opening more code cells and using the scikit-learn documentation to guide some further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to Submit?\n",
    "\n",
    "Please run your Jupyter Notebook first to generate outputs for each code cell and then export the report as a PDF file by clicking the following links (File -> Download as -> PDF vis HTML (.pdf)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
